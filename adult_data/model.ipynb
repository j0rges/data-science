{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling of adult data #\n",
    "\n",
    "I am going to test a few models to predict high income based on the features we have (race, sex, gender, etc).\n",
    "\n",
    "The models I am going to try are:\n",
    "\n",
    "1. Logistic regression\n",
    "1. Random Forest\n",
    "1. Naive Bayes\n",
    "\n",
    "I will transform the features slightly for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_data = pd.read_csv('data/adult.data', header=None)\n",
    "adult_test = pd.read_csv('data/adult.test', header=None, skiprows = 1)\n",
    "\n",
    "adult_data.columns = ['age', 'type_employer', 'fnlwgt', 'education', \n",
    "                \"education_num\",\"marital\", \"occupation\", \"relationship\", \"race\",\"sex\",\n",
    "                \"capital_gain\", \"capital_loss\", \"hr_per_week\",\"country\",\"income\"]\n",
    "adult_test.columns = ['age', 'type_employer', 'fnlwgt', 'education', \n",
    "                \"education_num\",\"marital\", \"occupation\", \"relationship\", \"race\",\"sex\",\n",
    "                \"capital_gain\", \"capital_loss\", \"hr_per_week\",\"country\",\"income\"]\n",
    "# Fix slightly different formating\n",
    "adult_test.replace([' <=50K.',' >50K.'],[' <=50K',' >50K'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the dataframe ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First divide between target and features:\n",
    "train_target = pd.get_dummies(adult_data, columns=['income'], drop_first=True)['income_ >50K']\n",
    "train_features = adult_data.drop(columns = ['income'])\n",
    "test_target = pd.get_dummies(adult_test, columns=['income'], drop_first=True)['income_ >50K']\n",
    "test_features = adult_test.drop(columns = ['income'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "class transform_adult_data():\n",
    "    \"\"\" Put the dataframe in the format we want for our model, following the convention of sklearn pipelines\n",
    "        ie implementing fit and transform.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.columns = None\n",
    "        pass\n",
    "    \n",
    "    def fit(self, df):\n",
    "        df = self.__transform(df)\n",
    "        self.columns = df.columns\n",
    "        return df\n",
    "    \n",
    "    def transform(self, df):\n",
    "        if self.columns is None:\n",
    "            raise NotFittedError('Call fit before using transform.')\n",
    "        result = pd.DataFrame(columns = self.columns)\n",
    "        result = result.append(self.__transform(df).fillna(0))\n",
    "        return result\n",
    "    \n",
    "    def __transform(self, df):\n",
    "        df = pd.get_dummies(df, columns=['sex'], drop_first=True)\n",
    "        #Â Education, race and high income are highly correlated, but the relationship isn't linear,\n",
    "        # race is categorical.\n",
    "        df = pd.get_dummies(df, columns=['education','race','type_employer','marital','occupation'])\n",
    "        # In EDA we saw that the trend for age reverses around the age of 50, therefore we will have turn age\n",
    "        # into two features, one for age over 50, the other for age under 50.\n",
    "        df['age >= 50'] = df['age'] * (df['age'] >= 50)\n",
    "        df['age < 50'] = df['age'] * (df['age'] < 50)\n",
    "        # Drop the columns that we won't be using as features (or the target):\n",
    "        df.drop(columns=['fnlwgt','education_num', 'relationship',\n",
    "                         'capital_gain','capital_loss','hr_per_week','country','age'], inplace=True)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561, 55)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing = transform_adult_data()\n",
    "train_data = processing.fit(train_features)\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16281, 55)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = processing.transform(test_features)\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 83.29%\n",
      "The confusion matrix is:\n",
      "[[11490   945]\n",
      " [ 1775  2071]]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train_data)\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(scaler.transform(train_data), train_target)\n",
    "\n",
    "prediction = model.predict(scaler.transform(test_data))\n",
    "\n",
    "acc = metrics.accuracy_score(test_target, prediction)\n",
    "confusion = metrics.confusion_matrix(test_target, prediction)\n",
    "\n",
    "print('The accuracy is {:.2f}%'.format(acc*100))\n",
    "print('The confusion matrix is:')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 81.20%\n",
      "The confusion matrix is:\n",
      "[[11078  1357]\n",
      " [ 1704  2142]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "model.fit(train_data, train_target)\n",
    "\n",
    "prediction = model.predict(test_data)\n",
    "\n",
    "acc = metrics.accuracy_score(test_target, prediction)\n",
    "confusion = metrics.confusion_matrix(test_target, prediction)\n",
    "print('The accuracy is {:.2f}%'.format(acc*100))\n",
    "print('The confusion matrix is:')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing ###\n",
    "\n",
    "I have decided to use Categorical Naive Bayes, therefore I am going to do slightly different pre-processing.\n",
    "\n",
    "Mainly not using dummies and binning age into 20 age groups (in the obvious manner). I will also keep some of the features I discarded for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_value(value, bins):\n",
    "    # assumes any value above last bin goes in last bin and below first bin goes in first bin.\n",
    "    for i in range(len(bins)-1):\n",
    "        if value <= bins[i+1]:\n",
    "            return i\n",
    "    return len(bins)-1\n",
    "\n",
    "class Bayes_transform():\n",
    "    def __init__(self, num_bins=20):\n",
    "        self.num_bins = num_bins\n",
    "        self.age_bins = None\n",
    "        \n",
    "    def fit(self, df, y=None):\n",
    "        self.age_bins = np.linspace(min(df['age']), max(df['age']), self.num_bins+1)\n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, df, y=None):\n",
    "        self.fit(df)\n",
    "        return self.transform(df)\n",
    "    \n",
    "    def transform(self, df):\n",
    "        if self.age_bins is None:\n",
    "            raise NotFittedError('Call fit before using transform.')\n",
    "        df = df.copy()\n",
    "        # Bin age into groups\n",
    "        df['age'] = df['age'].map(lambda x: bin_value(x, self.age_bins))\n",
    "        # Drop the columns that we won't be using as features (either continuous or not interesting):\n",
    "        df.drop(columns=['fnlwgt','education_num', 'capital_gain','capital_loss',\n",
    "                         'hr_per_week','country'], inplace=True)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model/Pipeline ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.naive_bayes import CategoricalNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 79.86%\n",
      "The confusion matrix is:\n",
      "[[10065  2370]\n",
      " [  909  2937]]\n"
     ]
    }
   ],
   "source": [
    "naive_bayes_pipeline = Pipeline([('feature_preparation', Bayes_transform(20)),\n",
    "                                 ('encoder', OrdinalEncoder()), \n",
    "                                 ('classifier', CategoricalNB())])\n",
    "\n",
    "naive_bayes_pipeline.fit(train_features, train_target)\n",
    "prediction = naive_bayes_pipeline.predict(test_features)\n",
    "\n",
    "acc = metrics.accuracy_score(test_target, prediction)\n",
    "confusion = metrics.confusion_matrix(test_target, prediction)\n",
    "print('The accuracy is {:.2f}%'.format(acc*100))\n",
    "print('The confusion matrix is:')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison ##\n",
    "\n",
    "The short answer is that between logistic regression and random forest, logistic regression does a better job. On the other hand, when it is not so straightforward to compare logistic regression and naive bayes, because although the accuracy of the former is better, the number of true positives in the later is much larger. I plan on having a look at a couple other metrics, but ultimately which model is better would be depend on the application, and which kind of error is less desirable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
